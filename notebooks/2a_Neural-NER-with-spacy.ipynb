{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural NER with spacy\n",
    "In this hands-on we use https://spacy.io, a framework for all basic NLP processing steps and support several languages out-of-the-box:\n",
    "\n",
    " - Tokenization/Word segmentation\n",
    " - Sentence splitting\n",
    " - Part-of-Speech tagging\n",
    " - Lemmatization\n",
    " - NER\n",
    " - Dependency Parsing\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands-on\n",
    "Downloading a small model for English trained on modern Web data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz (11.1MB)\n",
      "\u001b[K     |████████████████████████████████| 11.1MB 1.8MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: en-core-web-sm\n",
      "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-rwcqwuma/wheels/39/ea/3b/507f7df78be8631a7a3d7090962194cf55bc1158572c0be77f\n",
      "Successfully built en-core-web-sm\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-2.1.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up an NLP pipeline for English:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading and linguistic processing of a single sentence. More information on the meaning of the results can be found under https://spacy.io/usage/linguistic-features .\n",
    "Note that the human readable properties of word tokens always end with underscore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Apple PROPN NNP nsubj Xxxxx True False\n",
      "is be VERB VBZ aux xx True True\n",
      "looking look VERB VBG ROOT xxxx True False\n",
      "at at ADP IN prep xx True True\n",
      "buying buy VERB VBG pcomp xxxx True False\n",
      "U.K. U.K. PROPN NNP compound X.X. False False\n",
      "startup startup NOUN NN dobj xxxx True False\n",
      "for for ADP IN prep xxx True True\n",
      "$ $ SYM $ quantmod $ False False\n",
      "1 1 NUM CD compound d False False\n",
      "billion billion NUM CD pobj xxxx True False\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spacy default NLP pipeline for English includes NER. See https://spacy.io/usage/linguistic-features#named-entities for more information.\n",
    "\n",
    "Let's look at the text of all found named entities: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple 0 5 ORG\n",
      "U.K. 27 31 GPE\n",
      "$1 billion 44 54 MONEY\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing IOB information at the token level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apple', 'B', 'ORG']\n"
     ]
    }
   ],
   "source": [
    "print([doc[0].text, doc[0].ent_iob_, doc[0].ent_type_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', 'I', 'MONEY']\n"
     ]
    }
   ],
   "source": [
    "print([doc[9].text, doc[9].ent_iob_, doc[9].ent_type_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More information of accessing NER information: https://spacy.io/usage/linguistic-features#accessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with different language\n",
    "If you want to work on French data, use the following commands for setting up an NLP pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fr_core_news_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-2.1.0/fr_core_news_sm-2.1.0.tar.gz#egg=fr_core_news_sm==2.1.0\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-2.1.0/fr_core_news_sm-2.1.0.tar.gz (13.1MB)\n",
      "\u001b[K     |████████████████████████████████| 13.1MB 1.8MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: fr-core-news-sm\n",
      "  Building wheel for fr-core-news-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-n9i49rco/wheels/ab/82/2a/61dd0ff02e22f10eef65a5aa35453a0eb745c84b4c874b612f\n",
      "Successfully built fr-core-news-sm\n",
      "Installing collected packages: fr-core-news-sm\n",
      "Successfully installed fr-core-news-sm-2.1.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('fr_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\r\n",
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/fr_core_news_sm -->\r\n",
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/spacy/data/fr_core_news_sm\r\n",
      "You can now load the model via spacy.load('fr_core_news_sm')\r\n"
     ]
    }
   ],
   "source": [
    "# without this line spacy is not able to find the downloaded model\n",
    "\n",
    "! python -m spacy link --force fr_core_news_sm fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nlp_fr = spacy.load(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See https://spacy.io/usage/models for more languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the robustness\n",
    " - Try sentences from other domains than contemporary news.\n",
    " - Add noise (OCR errors, typos) to the text.\n",
    "\n",
    "How much do the results suffer?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next step: Combining rule-based and statistical NER\n",
    "A tutorial how to use rule-based pattern matchers in spacy can be found here: https://spacy.io/usage/rule-based-matching#entityruler\n",
    "A nice example is the rule-based addition of titles to named entities that have been recognized without the titles by statistical NER: https://spacy.io/usage/rule-based-matching#models-rules-ner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next step: Online training of existing models\n",
    "All spacy NER models can be updated easily by further training them on new labeled examples.\n",
    "The relevant documentation and sample code of spacy can be found here: https://spacy.io/usage/training#ner\n",
    "\n",
    "There is a step-by-step tutorial that shows how an existing model can be adapted to your own data:\n",
    "https://towardsdatascience.com/custom-named-entity-recognition-using-spacy-7140ebbb3718"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
