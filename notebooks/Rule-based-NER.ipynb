{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing a rule-based system\n",
    "\n",
    "### Context\n",
    "\n",
    "This notebook is meant to drive you through the usage of Corleone and Express, two softwares developed at the Joint Research Center (European Commission, Ispra, Italy) by Jakub Piskorski. These softwares are in use in production applications, notably the Europe Media Monitor. They are free for academic usage but you should ask a license if you want to use them beyond this tutorial.\n",
    "\n",
    "### Tools\n",
    "\n",
    "- **Corleone** (Core Linguistic Entity Online Extraction) is a set of lightweight linguistic processing components (text scanner, tokenizer, sentence splitter, morphological analysis and gazetteer lookup).\n",
    "- **Express** (Extraction Pattern Recognition Engine and Specification Suite) is an information extraction grammar engine, which consists of a grammar parser and a grammar interpreter.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "We will not manage to do build a full information extraction pipeline within the allocated time (we would need at leat a week!). Here the objective is to **give you an idea** of how things work. We will therefore focus on 2 components: Gazetteers and Grammar, trying to build a small engine to recognize (some) person names. We will rely on an already compiled tokeniser. You will develop a person name gazetteer, and 2 or 3 grammar rules relying on it. Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folder structure\n",
    "\n",
    "In both corleone and express repositories, you will find the following structures:\n",
    "\n",
    "```bash \n",
    ".\n",
    "├── compiled-resources # this is where your compiled resources will go\n",
    "├── documentation # user-guide is available here\n",
    "├── experiments # a playground folder, already with some inputs\n",
    "│   ├── input\n",
    "│   └── output\n",
    "├── resources # the 'row' resources, i.e. gazetteers and grammar file before they get compiled\n",
    "└── scripts # the scripts to use to compile or apply the components\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the JARs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the URL to insert in this variable will be communicated\n",
    "# during the workshop as the tool license does not allow us to\n",
    "# further distribute it (i.e. putting it on GitHub)\n",
    "download_link = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget {download_link} -O ../libraries.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip ../libraries.zip -d ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ../librairies/*.jar ../rule-based/lib/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -al ../rule-based/lib/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up\n",
    "! rm -r ../librairies/\n",
    "! rm libraries.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. CORLEONE: creating, compiling and applying a gazetteer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first exercise, we will create a small gazetteer for person names.\n",
    "\n",
    "> The CorLEONE gazetteer look-up (dictionary look-up) component matches an input stream of characters or tokens against a gazetteer (dictionary) list, and produces an adequate annotation for the matched text fragment. It allows for associating each entry in the gazetteer with a list of arbitrary flat attribute-value pairs*. (Corleone documentation, Piskorski, 2018.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Creating a person name gazetteer\n",
    "\n",
    "The resources you need to manipulate are under the repository `resources`:\n",
    "\n",
    "- The **raw gazetteer file**, e.g. `person_name_gazetteer.txt`, is the entry file you need to edit with gazetteers elements. Each line represents a single gazetteer entry in the following format: `keyword (attribute:value)+`. \n",
    "\n",
    "```bash\n",
    "# Example of gazetteer, one entry per line, where the input separator is \"|\",\n",
    "# and the attribute/value separator is \":\"\n",
    "New York | GTYPE:location | SUBTYPE:city | CONTINENT: north america\n",
    "G. Bush  | GTYPE: person | SUBTYPE: politician | position: president \n",
    "# => here we are declaring that the string \"New York\" has the GTYPE 'location', the SUBTYPE 'city', etc.\n",
    "\n",
    "# for ambiguous forms, one line per referent:\n",
    "Washington | GTYPE:city | LOCATION:USA | SUBTYPE:cap_city \n",
    "Washington | GTYPE:person | GENDER:m_f \n",
    "Washington | GTYPE:organization | SUBTYPE:commercial \n",
    "Washington | GTYPE:region | LOCATION:US\n",
    "```\n",
    "\n",
    "- The **attribute file** lists all attribute names, where each line stands for a single attribute name. \n",
    "\n",
    "  ```bash\n",
    "  # for our gazetteer above, we need to declare the following types:\n",
    "  GTYPE\n",
    "  SUBTYPE\n",
    "  CONTINENT\n",
    "  LOCATION\n",
    "  GENDER\n",
    "  # => this are the types with which we want to describe our gazetteer entries\n",
    "  ```\n",
    "  \n",
    "- The **type file** (optional) can be used in order to facilitate more strict encoding of the gazetteer entries in order to specify: (a) an attribute that is used to encode the type of the entry, which has to be provided in all entries in the entry file, and (b) a list of appropriate attributes for each type.\n",
    "\n",
    "  ```bash\n",
    "  # for our example above, the type file would contain:\n",
    "  GTYPE # means that all entries need this type, and they can have 'city', 'person' or 'region' as values\n",
    "  city location subtype # means that if an entry is of GTYPE city, it can have the 'location' and 'subtype' attributes\n",
    "  person gender subtype position \n",
    "  region location\n",
    "  \n",
    "  # this is more specific than the type file, this is to declare the possible values for each type.\n",
    "  # this is not mandatory, we can skip it for our exercise.\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Compiling a person name gazetteer \n",
    "\n",
    "A very small person name gazetteer already exists. Let's try to use it.\n",
    "\n",
    "`<digression type'short'>`\n",
    "\n",
    "**What's going on?** In the cells below we are using a special syntax to run **bash** commands (e.g. `cd`, `ls -la`, etc.) **from within** the notebook.\n",
    "\n",
    "By using the `!` prefix at the beginning of a line we tell Jupyter than the line content should be interpreted and executed as a bash command (rather than a Python statement).\n",
    "\n",
    "`</digression>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_HOME = \"~/notebooks/\"\n",
    "CORLEONE_RESOURCES_DIR = \"/home/jovyan/rule-based/corleone/resources/\"\n",
    "CORLEONE_SCRIPTS_DIR = \"/home/jovyan/rule-based/corleone/scripts/\"\n",
    "CORLEONE_COMPILED_RESOURCES = \"/home/jovyan/rule-based/corleone/compiled-resources/\"\n",
    "EXPERIMENTS_OUTPUT_DIR = \"/home/jovyan/rule-based/corleone/experiments/output/\"\n",
    "# TODO: add other dirs here below and change to ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go in the resource folder and look at the person_name_gazetteer.txt file\n",
    "os.chdir(CORLEONE_RESOURCES_DIR)\n",
    "! ls -la\n",
    "! head person_name_gazetteer.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the configuration files if you wish: they are already ready, you do not need to edit them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go in the /scripts folder of corleone\n",
    "os.chdir(CORLEONE_SCRIPTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute the 'compile component script' with the component alias (basicGazetteer)\n",
    "# and the component configuration file (located in the resource folder)\n",
    "\n",
    "component_alias = \"basicGazetteer\"\n",
    "component_config_file = \"../resources/person_nameGazetteer.cfg\"\n",
    "\n",
    "! ./compileComp.sh {component_alias} {component_config_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go in the compiled resources and check if your compiled component is there\n",
    "os.chdir(CORLEONE_COMPILED_RESOURCES)\n",
    "! ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Apply the compiled gazetteer to some inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go in the /scripts folder of corleone\n",
    "\n",
    "os.chdir(CORLEONE_SCRIPTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute the 'apply component script' with the component alias\n",
    "# (basicGazetteer) and the component configuration file (located in the resource folder)\n",
    "\n",
    "! ./applyComp.sh basicGazetteer ../resources/person_name_gaz_application.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go in the experiment folder and check the output\n",
    "\n",
    "! head '../experiments/output/\\person_gazetteer_input.txt.out'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Iterate \n",
    "\n",
    "Now that you did the first edit-compiling-applying cycle, you can go back to the entry file and add more entries. The information you enter will be used by the grammar rules that you will develop next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Express: Creating, Compiling and Applying a grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Some explanations\n",
    "\n",
    "\n",
    "For now we will apply a grammar containing 2 rules, which you can already use. The explainations below are to give you some information.\n",
    "\n",
    "A grammar cascade definition consists of three main ingredients:\n",
    "\n",
    "#### 1. Definition of feature structure types*\n",
    "\n",
    "The type specification file contains the types and their attributes which are used in the grammars.\n",
    "It is in this file that you define that you want to create a type 'person' or 'organisation', with their attributes 'firstname' and 'headquarters', for examples.\n",
    "\n",
    "Express has an interface with Corleone components and is able to \"understand\" some of Corleone types by default. For example, Corleone tokenizer has the type \"token\" with the attributes \"type\" and \"surface\".\n",
    "\n",
    "Given our person name gazetteers and the Corleone modules that we use, we define our grammar type file as follows:\n",
    "\n",
    "```\n",
    "basic-token := [SURFACE] # coming from Corleone component\n",
    "token := [TYPE,SURFACE] # coming from Corleone component, a more refined tokenizer\n",
    "person := [NAME,FIRST_NAME,LAST_NAME,INITIAL,TITLE,RULE] # the type we want to manipulate in our preons grammar file\n",
    "gaz_given_name := [SURFACE] # coming from the person gazetteer\n",
    "gaz_title := [GNUMBER,SURFACE] # coming from the person gazetteer\n",
    "gaz_initial := [SURFACE] # coming from the person gazetteer\n",
    "gaz_name_infix := [SURFACE] # coming from the person gazetteer\n",
    "```\n",
    "\n",
    "This file type is already defined in the folder structure of the hands-on, normally there is not need to change nothing (unless you add more types in our gazetteer).\n",
    "\n",
    "#### 2. Set of grammar specifications\n",
    "\n",
    "\n",
    "This is the grammar file per se, which consists of 2 parts:\n",
    "\n",
    "**A. Setting part** \n",
    "\n",
    "**Normally you do not need to change the setting part for the hands-on.**\n",
    "\n",
    "- MODULES: to specify the list of pre-processing modules which will be applied before the grammar interpreter. In our case, we use Corleone modules, including the person gazetteer compiled in the previous step. These pre-processing components provide the grammar interpreter with a stream of input feature structures.\n",
    "\n",
    "- SEARCH_MODE: defines the matchin strategy. Here we choose \"longest match\".\n",
    "\n",
    "- OUTPUT: defines what the interpreter should outputs, its own feature structures (grammar), all the feature structures of all applied components (all), all the feature structures of applied and non-applied conponents (grammar_and_unconsumed).\n",
    "\n",
    "\n",
    "**B. Rule part**\n",
    "\n",
    "**This is the part you might want to update**\n",
    "\n",
    "The part between **PATTERNS** and **END_PATTERNS** contains the rule definitions.\n",
    "\n",
    "#### 3. Set of **rule prioritisation** definition\n",
    "We do not use this part in the hands-on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Using a grammar\n",
    "\n",
    "A grammar already exists in our folder, we will try to use it.\n",
    "\n",
    "**Let's first have a look at the grammar file:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining our paths\n",
    "EXPRESS_RESOURCES_DIR = os.path.expanduser(\"~/rule-based/express/resources/\")\n",
    "EXPRESS_SCRIPTS_DIR = os.path.expanduser(\"~/rule-based/express/scripts/\")\n",
    "EXPRESS_COMPILED_RESOURCES = os.path.expanduser(\"~/rule-based/express/compiled-resources/\")\n",
    "EXPRESS_OUTPUT_DIR = os.path.expanduser(\"~/rule-based/express/experiments/output/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we go in the express resource folder\n",
    "os.chdir(EXPRESS_RESOURCES_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we look at the rule file\n",
    "! cat grammar_person_rules_0.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's check the resources of the grammar*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we copy our compiled person gazetteer \".gaz\" into express \n",
    "# compiled resources folder, so that the grammar interpreter an use it.\n",
    "\n",
    "cp corleone/compiled-resources/person_names.gaz express/compiled-resources/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's list our compiled resource folder:**\n",
    "``` bash\n",
    ".\n",
    "├── data # contains the compiled resources, this is where the person gaz and grammar compiled files go.\n",
    "│   ├── BasicTokenizer.btk\n",
    "│   ├── ClassifyingTokenizer_EN.tok\n",
    "│   ├── grammar_person.grm\n",
    "│   ├── person_grammar_types.txt\n",
    "│   └── person_names.gaz\n",
    "└── modules_grammar # contains the configuration for the pre-processing modules, no need to change\n",
    "    ├── basic_tokenizer_configuration.cfg\n",
    "    ├── classifying_tokenizer_configuration.cfg\n",
    "    └── gazetteerPersonConfig.cfg\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's compile our grammar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(EXPRESS_SCRIPTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "./runParser.sh ../resources/compilation.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's see if the grammar file *.grm is in the compiled resources folder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll ../compiled-resources/data/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can now apply our grammar on texts**\n",
    "\n",
    "The input folder in experiments contains a small file with some person names. Let's open it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "less ../experiments/input/grammar_person_input.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's try to apply the grammar on this file first**. The output will appear on the experiment/output folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "./runInterpreter.sh ../resources/execution.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's observe the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "less ../experiments/output\\grammar_person_input.txt-result.txt # here there is the same pb with the \"\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What you can try next:**\n",
    "\n",
    "- After this first application, try to change the OUTPUT attribute from 'grammar' to 'all' in the grammar file, recompile the grammar and apply it again on the same file. You should see more information in the output file.\n",
    "- move more input files in the \"input\" folder (some examples in different languages are just one level up), and observe the results\n",
    "- try to write a rule to recognize the missed names in ``\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
